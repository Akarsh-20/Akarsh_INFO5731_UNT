{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akarsh-20/Akarsh_INFO5731_UNT/blob/main/Doddi_Akarsh_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_imdb_reviews(movie_id, num_reviews=10000):\n",
        "    base_url = f'https://www.imdb.com/title/{movie_id}/reviews?ref_=tt_ql_3'\n",
        "\n",
        "    reviews = []\n",
        "    page_number = 1\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        url = f\"{base_url}&start={page_number}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_elements = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        if not review_elements:\n",
        "            break\n",
        "\n",
        "        for review in review_elements:\n",
        "            reviews.append(review.text.strip())\n",
        "\n",
        "        page_number += 1\n",
        "\n",
        "    return reviews[:num_reviews]\n",
        "\n",
        "def save_reviews_to_csv(data, filename='imdb_reviews.csv'):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(['Review'])\n",
        "        writer.writerows([[review] for review in data])\n",
        "\n",
        "# Here you can change the movie ID\n",
        "movie_id = 'tt15398776'\n",
        "num_reviews_to_scrape = 10000\n",
        "\n",
        "movie_reviews = scrape_imdb_reviews(movie_id, num_reviews_to_scrape)\n",
        "save_reviews_to_csv(movie_reviews)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0feca7-39f5-4a7f-ace3-9b8096f8d7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# importing required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    # (1) Remove noise (special characters and punctuations)\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "\n",
        "    # (2) Remove numbers\n",
        "    text = ''.join([char for char in text if not char.isdigit()])\n",
        "\n",
        "    # (3) Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([word for word in tokens if word.lower() not in stop_words])\n",
        "\n",
        "    # (4) Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # (5) Stemming\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([porter_stemmer.stem(word) for word in tokens])\n",
        "\n",
        "    # (6) Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
        "\n",
        "    return text\n",
        "\n",
        "def scrape_imdb_reviews(movie_id, num_reviews=10000):\n",
        "    base_url = f'https://www.imdb.com/title/{movie_id}/reviews?ref_=tt_ql_3'\n",
        "\n",
        "    reviews = []\n",
        "    page_number = 1\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        url = f\"{base_url}&start={page_number}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_elements = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        if not review_elements:\n",
        "            break\n",
        "\n",
        "        for review in review_elements:\n",
        "            cleaned_review = clean_text(review.text.strip())\n",
        "            reviews.append(cleaned_review)\n",
        "\n",
        "        page_number += 1\n",
        "\n",
        "    return reviews[:num_reviews]\n",
        "\n",
        "def save_reviews_to_csv(data, filename='imdb_reviews_cleaned.csv'):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(['Original Review', 'Cleaned Review'])\n",
        "        for original_review, cleaned_review in zip(data, [clean_text(review) for review in data]):\n",
        "            writer.writerow([original_review, cleaned_review])\n",
        "\n",
        "# Replace 'your_movie_id' with the actual IMDb movie ID\n",
        "movie_reviews = scrape_imdb_reviews('tt15398776', num_reviews=10000)\n",
        "save_reviews_to_csv(movie_reviews)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f42f965-e933-4c50-e5ff-af849dc8874f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parts of Speech (POS) Tagging:\n",
            "[('youll', 'NN'), ('wit', 'NN'), ('brain', 'NN'), ('fulli', 'JJ'), ('switch', 'NN'), ('watch', 'NN'), ('oppenheim', 'NN'), ('could', 'MD'), ('easili', 'VB'), ('get', 'VB'), ('away', 'RB'), ('nonatt', 'JJ'), ('viewer', 'NN'), ('intellig', 'NN'), ('filmmak', 'NN'), ('show', 'NN'), ('audienc', 'VBZ'), ('great', 'JJ'), ('respect', 'JJ'), ('fire', 'NN'), ('dialogu', 'NN'), ('pack', 'NN'), ('inform', 'NN'), ('relentless', 'NN'), ('pace', 'NN'), ('jump', 'NN'), ('differ', 'NN'), ('time', 'NN'), ('oppenheim', 'JJ'), ('life', 'NN'), ('continu', 'VB'), ('hour', 'NN'), ('runtim', 'NN'), ('visual', 'JJ'), ('clue', 'NN'), ('guid', 'NN'), ('viewer', 'NN'), ('time', 'NN'), ('youll', 'JJ'), ('get', 'NN'), ('grip', 'NN'), ('quit', 'NN'), ('quickli', 'NN'), ('relentless', 'NN'), ('help', 'NN'), ('express', 'VB'), ('urgenc', 'JJ'), ('u', 'JJ'), ('attack', 'NN'), ('chase', 'NN'), ('atom', 'NN'), ('bomb', 'NN'), ('germani', 'NN'), ('could', 'MD'), ('absolut', 'VB'), ('career', 'NN'), ('best', 'JJS'), ('perform', 'NN'), ('consistenli', 'NN'), ('brilliant', 'JJ'), ('cillian', 'JJ'), ('murphi', 'NN'), ('anchor', 'NN'), ('film', 'NN'), ('nail', 'NN'), ('oscar', 'NN'), ('perform', 'NN'), ('fact', 'NN'), ('whole', 'JJ'), ('cast', 'NN'), ('fantast', 'JJ'), ('apart', 'RB'), ('mayb', 'VBP'), ('sometim', 'JJ'), ('overwrought', 'JJ'), ('emili', 'NN'), ('blunt', 'NN'), ('perform', 'NN'), ('rdj', 'NN'), ('also', 'RB'), ('particularli', 'JJ'), ('brilliant', 'JJ'), ('return', 'NN'), ('proper', 'JJ'), ('act', 'NN'), ('decad', 'NN'), ('call', 'NN'), ('screenplay', 'NN'), ('den', 'NN'), ('layer', 'NN'), ('id', 'NNS'), ('say', 'VBP'), ('thick', 'JJ'), ('bibl', 'NN'), ('cinematographi', 'NN'), ('quit', 'NN'), ('stark', 'NN'), ('spare', 'JJ'), ('part', 'NN'), ('imbu', 'NN'), ('rich', 'JJ'), ('luciou', 'VBZ'), ('colour', 'JJ'), ('moment', 'NN'), ('especi', 'JJ'), ('scene', 'NN'), ('florenc', 'NN'), ('pugh', 'IN'), ('score', 'NN'), ('beauti', 'NN'), ('time', 'NN'), ('mostli', 'JJ'), ('anxiou', 'RB'), ('oppress', 'JJ'), ('ad', 'NN'), ('relentless', 'NN'), ('pace', 'NN'), ('hour', 'NN'), ('runtim', 'NN'), ('fli', 'NN'), ('found', 'VBD'), ('inten', 'JJ'), ('tax', 'NN'), ('highli', 'NN'), ('reward', 'NN'), ('watch', 'NN'), ('film', 'NN'), ('make', 'VBP'), ('finest', 'JJS'), ('realli', 'JJ'), ('great', 'JJ'), ('watch', 'NN')]\n",
            "POS Counts: Counter({'NN': 83, 'JJ': 28, 'VB': 5, 'RB': 4, 'VBP': 3, 'MD': 2, 'VBZ': 2, 'JJS': 2, 'NNS': 1, 'IN': 1, 'VBD': 1})\n",
            "Constituency Parsing Tree:\n",
            "(S\n",
            "  youll/NN\n",
            "  wit/NN\n",
            "  brain/NN\n",
            "  fulli/JJ\n",
            "  switch/NN\n",
            "  watch/NN\n",
            "  oppenheim/NN\n",
            "  could/MD\n",
            "  easili/VB\n",
            "  get/VB\n",
            "  away/RB\n",
            "  nonatt/JJ\n",
            "  viewer/NN\n",
            "  intellig/NN\n",
            "  filmmak/NN\n",
            "  show/NN\n",
            "  audienc/VBZ\n",
            "  great/JJ\n",
            "  respect/JJ\n",
            "  fire/NN\n",
            "  dialogu/NN\n",
            "  pack/NN\n",
            "  inform/NN\n",
            "  relentless/NN\n",
            "  pace/NN\n",
            "  jump/NN\n",
            "  differ/NN\n",
            "  time/NN\n",
            "  oppenheim/JJ\n",
            "  life/NN\n",
            "  continu/VB\n",
            "  hour/NN\n",
            "  runtim/NN\n",
            "  visual/JJ\n",
            "  clue/NN\n",
            "  guid/NN\n",
            "  viewer/NN\n",
            "  time/NN\n",
            "  youll/JJ\n",
            "  get/NN\n",
            "  grip/NN\n",
            "  quit/NN\n",
            "  quickli/NN\n",
            "  relentless/NN\n",
            "  help/NN\n",
            "  express/VB\n",
            "  urgenc/JJ\n",
            "  u/JJ\n",
            "  attack/NN\n",
            "  chase/NN\n",
            "  atom/NN\n",
            "  bomb/NN\n",
            "  germani/NN\n",
            "  could/MD\n",
            "  absolut/VB\n",
            "  career/NN\n",
            "  best/JJS\n",
            "  perform/NN\n",
            "  consistenli/NN\n",
            "  brilliant/JJ\n",
            "  cillian/JJ\n",
            "  murphi/NN\n",
            "  anchor/NN\n",
            "  film/NN\n",
            "  nail/NN\n",
            "  oscar/NN\n",
            "  perform/NN\n",
            "  fact/NN\n",
            "  whole/JJ\n",
            "  cast/NN\n",
            "  fantast/JJ\n",
            "  apart/RB\n",
            "  mayb/VBP\n",
            "  sometim/JJ\n",
            "  overwrought/JJ\n",
            "  emili/NN\n",
            "  blunt/NN\n",
            "  perform/NN\n",
            "  rdj/NN\n",
            "  also/RB\n",
            "  particularli/JJ\n",
            "  brilliant/JJ\n",
            "  return/NN\n",
            "  proper/JJ\n",
            "  act/NN\n",
            "  decad/NN\n",
            "  call/NN\n",
            "  screenplay/NN\n",
            "  den/NN\n",
            "  layer/NN\n",
            "  id/NNS\n",
            "  say/VBP\n",
            "  thick/JJ\n",
            "  bibl/NN\n",
            "  cinematographi/NN\n",
            "  quit/NN\n",
            "  stark/NN\n",
            "  spare/JJ\n",
            "  part/NN\n",
            "  imbu/NN\n",
            "  rich/JJ\n",
            "  luciou/VBZ\n",
            "  colour/JJ\n",
            "  moment/NN\n",
            "  especi/JJ\n",
            "  scene/NN\n",
            "  florenc/NN\n",
            "  pugh/IN\n",
            "  score/NN\n",
            "  beauti/NN\n",
            "  time/NN\n",
            "  mostli/JJ\n",
            "  anxiou/RB\n",
            "  oppress/JJ\n",
            "  ad/NN\n",
            "  relentless/NN\n",
            "  pace/NN\n",
            "  hour/NN\n",
            "  runtim/NN\n",
            "  fli/NN\n",
            "  found/VBD\n",
            "  inten/JJ\n",
            "  tax/NN\n",
            "  highli/NN\n",
            "  reward/NN\n",
            "  watch/NN\n",
            "  film/NN\n",
            "  make/VBP\n",
            "  finest/JJS\n",
            "  realli/JJ\n",
            "  great/JJ\n",
            "  watch/NN)\n",
            "Dependency Parsing Tree:\n",
            "('youll', 'NN', 'O')\n",
            "('wit', 'NN', 'O')\n",
            "('brain', 'NN', 'O')\n",
            "('fulli', 'JJ', 'O')\n",
            "('switch', 'NN', 'O')\n",
            "('watch', 'NN', 'O')\n",
            "('oppenheim', 'NN', 'O')\n",
            "('could', 'MD', 'O')\n",
            "('easili', 'VB', 'O')\n",
            "('get', 'VB', 'O')\n",
            "('away', 'RB', 'O')\n",
            "('nonatt', 'JJ', 'O')\n",
            "('viewer', 'NN', 'O')\n",
            "('intellig', 'NN', 'O')\n",
            "('filmmak', 'NN', 'O')\n",
            "('show', 'NN', 'O')\n",
            "('audienc', 'VBZ', 'O')\n",
            "('great', 'JJ', 'O')\n",
            "('respect', 'JJ', 'O')\n",
            "('fire', 'NN', 'O')\n",
            "('dialogu', 'NN', 'O')\n",
            "('pack', 'NN', 'O')\n",
            "('inform', 'NN', 'O')\n",
            "('relentless', 'NN', 'O')\n",
            "('pace', 'NN', 'O')\n",
            "('jump', 'NN', 'O')\n",
            "('differ', 'NN', 'O')\n",
            "('time', 'NN', 'O')\n",
            "('oppenheim', 'JJ', 'O')\n",
            "('life', 'NN', 'O')\n",
            "('continu', 'VB', 'O')\n",
            "('hour', 'NN', 'O')\n",
            "('runtim', 'NN', 'O')\n",
            "('visual', 'JJ', 'O')\n",
            "('clue', 'NN', 'O')\n",
            "('guid', 'NN', 'O')\n",
            "('viewer', 'NN', 'O')\n",
            "('time', 'NN', 'O')\n",
            "('youll', 'JJ', 'O')\n",
            "('get', 'NN', 'O')\n",
            "('grip', 'NN', 'O')\n",
            "('quit', 'NN', 'O')\n",
            "('quickli', 'NN', 'O')\n",
            "('relentless', 'NN', 'O')\n",
            "('help', 'NN', 'O')\n",
            "('express', 'VB', 'O')\n",
            "('urgenc', 'JJ', 'O')\n",
            "('u', 'JJ', 'O')\n",
            "('attack', 'NN', 'O')\n",
            "('chase', 'NN', 'O')\n",
            "('atom', 'NN', 'O')\n",
            "('bomb', 'NN', 'O')\n",
            "('germani', 'NN', 'O')\n",
            "('could', 'MD', 'O')\n",
            "('absolut', 'VB', 'O')\n",
            "('career', 'NN', 'O')\n",
            "('best', 'JJS', 'O')\n",
            "('perform', 'NN', 'O')\n",
            "('consistenli', 'NN', 'O')\n",
            "('brilliant', 'JJ', 'O')\n",
            "('cillian', 'JJ', 'O')\n",
            "('murphi', 'NN', 'O')\n",
            "('anchor', 'NN', 'O')\n",
            "('film', 'NN', 'O')\n",
            "('nail', 'NN', 'O')\n",
            "('oscar', 'NN', 'O')\n",
            "('perform', 'NN', 'O')\n",
            "('fact', 'NN', 'O')\n",
            "('whole', 'JJ', 'O')\n",
            "('cast', 'NN', 'O')\n",
            "('fantast', 'JJ', 'O')\n",
            "('apart', 'RB', 'O')\n",
            "('mayb', 'VBP', 'O')\n",
            "('sometim', 'JJ', 'O')\n",
            "('overwrought', 'JJ', 'O')\n",
            "('emili', 'NN', 'O')\n",
            "('blunt', 'NN', 'O')\n",
            "('perform', 'NN', 'O')\n",
            "('rdj', 'NN', 'O')\n",
            "('also', 'RB', 'O')\n",
            "('particularli', 'JJ', 'O')\n",
            "('brilliant', 'JJ', 'O')\n",
            "('return', 'NN', 'O')\n",
            "('proper', 'JJ', 'O')\n",
            "('act', 'NN', 'O')\n",
            "('decad', 'NN', 'O')\n",
            "('call', 'NN', 'O')\n",
            "('screenplay', 'NN', 'O')\n",
            "('den', 'NN', 'O')\n",
            "('layer', 'NN', 'O')\n",
            "('id', 'NNS', 'O')\n",
            "('say', 'VBP', 'O')\n",
            "('thick', 'JJ', 'O')\n",
            "('bibl', 'NN', 'O')\n",
            "('cinematographi', 'NN', 'O')\n",
            "('quit', 'NN', 'O')\n",
            "('stark', 'NN', 'O')\n",
            "('spare', 'JJ', 'O')\n",
            "('part', 'NN', 'O')\n",
            "('imbu', 'NN', 'O')\n",
            "('rich', 'JJ', 'O')\n",
            "('luciou', 'VBZ', 'O')\n",
            "('colour', 'JJ', 'O')\n",
            "('moment', 'NN', 'O')\n",
            "('especi', 'JJ', 'O')\n",
            "('scene', 'NN', 'O')\n",
            "('florenc', 'NN', 'O')\n",
            "('pugh', 'IN', 'O')\n",
            "('score', 'NN', 'O')\n",
            "('beauti', 'NN', 'O')\n",
            "('time', 'NN', 'O')\n",
            "('mostli', 'JJ', 'O')\n",
            "('anxiou', 'RB', 'O')\n",
            "('oppress', 'JJ', 'O')\n",
            "('ad', 'NN', 'O')\n",
            "('relentless', 'NN', 'O')\n",
            "('pace', 'NN', 'O')\n",
            "('hour', 'NN', 'O')\n",
            "('runtim', 'NN', 'O')\n",
            "('fli', 'NN', 'O')\n",
            "('found', 'VBD', 'O')\n",
            "('inten', 'JJ', 'O')\n",
            "('tax', 'NN', 'O')\n",
            "('highli', 'NN', 'O')\n",
            "('reward', 'NN', 'O')\n",
            "('watch', 'NN', 'O')\n",
            "('film', 'NN', 'O')\n",
            "('make', 'VBP', 'O')\n",
            "('finest', 'JJS', 'O')\n",
            "('realli', 'JJ', 'O')\n",
            "('great', 'JJ', 'O')\n",
            "('watch', 'NN', 'O')\n",
            "\n",
            "Named Entity Recognition:\n",
            "Counter({('watch', 'NN'): 3, ('relentless', 'NN'): 3, ('time', 'NN'): 3, ('perform', 'NN'): 3, ('could', 'MD'): 2, ('viewer', 'NN'): 2, ('great', 'JJ'): 2, ('pace', 'NN'): 2, ('hour', 'NN'): 2, ('runtim', 'NN'): 2, ('quit', 'NN'): 2, ('brilliant', 'JJ'): 2, ('film', 'NN'): 2, ('youll', 'NN'): 1, ('wit', 'NN'): 1, ('brain', 'NN'): 1, ('fulli', 'JJ'): 1, ('switch', 'NN'): 1, ('oppenheim', 'NN'): 1, ('easili', 'VB'): 1, ('get', 'VB'): 1, ('away', 'RB'): 1, ('nonatt', 'JJ'): 1, ('intellig', 'NN'): 1, ('filmmak', 'NN'): 1, ('show', 'NN'): 1, ('audienc', 'VBZ'): 1, ('respect', 'JJ'): 1, ('fire', 'NN'): 1, ('dialogu', 'NN'): 1, ('pack', 'NN'): 1, ('inform', 'NN'): 1, ('jump', 'NN'): 1, ('differ', 'NN'): 1, ('oppenheim', 'JJ'): 1, ('life', 'NN'): 1, ('continu', 'VB'): 1, ('visual', 'JJ'): 1, ('clue', 'NN'): 1, ('guid', 'NN'): 1, ('youll', 'JJ'): 1, ('get', 'NN'): 1, ('grip', 'NN'): 1, ('quickli', 'NN'): 1, ('help', 'NN'): 1, ('express', 'VB'): 1, ('urgenc', 'JJ'): 1, ('u', 'JJ'): 1, ('attack', 'NN'): 1, ('chase', 'NN'): 1, ('atom', 'NN'): 1, ('bomb', 'NN'): 1, ('germani', 'NN'): 1, ('absolut', 'VB'): 1, ('career', 'NN'): 1, ('best', 'JJS'): 1, ('consistenli', 'NN'): 1, ('cillian', 'JJ'): 1, ('murphi', 'NN'): 1, ('anchor', 'NN'): 1, ('nail', 'NN'): 1, ('oscar', 'NN'): 1, ('fact', 'NN'): 1, ('whole', 'JJ'): 1, ('cast', 'NN'): 1, ('fantast', 'JJ'): 1, ('apart', 'RB'): 1, ('mayb', 'VBP'): 1, ('sometim', 'JJ'): 1, ('overwrought', 'JJ'): 1, ('emili', 'NN'): 1, ('blunt', 'NN'): 1, ('rdj', 'NN'): 1, ('also', 'RB'): 1, ('particularli', 'JJ'): 1, ('return', 'NN'): 1, ('proper', 'JJ'): 1, ('act', 'NN'): 1, ('decad', 'NN'): 1, ('call', 'NN'): 1, ('screenplay', 'NN'): 1, ('den', 'NN'): 1, ('layer', 'NN'): 1, ('id', 'NNS'): 1, ('say', 'VBP'): 1, ('thick', 'JJ'): 1, ('bibl', 'NN'): 1, ('cinematographi', 'NN'): 1, ('stark', 'NN'): 1, ('spare', 'JJ'): 1, ('part', 'NN'): 1, ('imbu', 'NN'): 1, ('rich', 'JJ'): 1, ('luciou', 'VBZ'): 1, ('colour', 'JJ'): 1, ('moment', 'NN'): 1, ('especi', 'JJ'): 1, ('scene', 'NN'): 1, ('florenc', 'NN'): 1, ('pugh', 'IN'): 1, ('score', 'NN'): 1, ('beauti', 'NN'): 1, ('mostli', 'JJ'): 1, ('anxiou', 'RB'): 1, ('oppress', 'JJ'): 1, ('ad', 'NN'): 1, ('fli', 'NN'): 1, ('found', 'VBD'): 1, ('inten', 'JJ'): 1, ('tax', 'NN'): 1, ('highli', 'NN'): 1, ('reward', 'NN'): 1, ('make', 'VBP'): 1, ('finest', 'JJS'): 1, ('realli', 'JJ'): 1})\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.chunk import tree2conlltags\n",
        "from collections import Counter\n",
        "\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in pos_tags)\n",
        "    return pos_tags, pos_counts\n",
        "\n",
        "def constituency_parsing(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tagged = pos_tag(words)\n",
        "        parsing_tree = ne_chunk(tagged)\n",
        "        print(\"Constituency Parsing Tree:\")\n",
        "        print(parsing_tree)\n",
        "\n",
        "def dependency_parsing(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tagged = pos_tag(words)\n",
        "        parsing_tree = ne_chunk(tagged)\n",
        "        conll_tags = tree2conlltags(parsing_tree)\n",
        "        print(\"Dependency Parsing Tree:\")\n",
        "        for tag in conll_tags:\n",
        "            print(tag)\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    entities = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tagged = pos_tag(words)\n",
        "        parsing_tree = ne_chunk(tagged, binary=True)\n",
        "        entities.extend([(word, entity) for word, entity, tag in tree2conlltags(parsing_tree) if entity != 'O'])\n",
        "    entity_counts = Counter(entities)\n",
        "    return entity_counts\n",
        "\n",
        "# Load cleaned reviews from the CSV file\n",
        "cleaned_reviews = []\n",
        "with open('imdb_reviews_cleaned.csv', 'r', encoding='utf-8') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        cleaned_reviews.append(row[1])\n",
        "\n",
        "# Performing analyses on a sample review\n",
        "sample_review = cleaned_reviews[0]\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "pos_tags, pos_counts = pos_tagging(sample_review)\n",
        "print(\"\\nParts of Speech (POS) Tagging:\")\n",
        "print(pos_tags)\n",
        "print(\"POS Counts:\", pos_counts)\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing\n",
        "constituency_parsing(sample_review)\n",
        "dependency_parsing(sample_review)\n",
        "\n",
        "# (3) Named Entity Recognition\n",
        "entity_counts = named_entity_recognition(sample_review)\n",
        "print(\"\\nNamed Entity Recognition:\")\n",
        "print(entity_counts)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**\n",
        "\n",
        "Constituency parsing and dependency parsing are two methods used in natural language processing to extract syntactic information from sentences.\n",
        "\n",
        "**Constituency Parsing**:\n",
        "\n",
        "Based on the formalism of context-free grammars\n",
        "\n",
        "Divide the sentence into constituents, which are sub-phrases that belong to a specific category in the grammar\n",
        "\n",
        "The parse tree includes sentences broken into sub-phrases, each belonging to a grammar category\n",
        "\n",
        "Focuses on the hierarchial structure of the sentence\n",
        "\n",
        "Can be used in word processing systems for grammar checking\n",
        "\n",
        "**DEPENDENCY PARSING**:\n",
        "\n",
        "Based on dependencies between words in a sentence\n",
        "\n",
        "The parse tree connects words according to their relationships\n",
        "\n",
        "Focuses on the linear structure of the sentence\n",
        "\n",
        "Can be more useful for several downstream tasks like information extraction or question answering\n",
        "\n",
        "Can be used to extract subject-verb-object triples that are often indicative of semantic relations between predicates\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}