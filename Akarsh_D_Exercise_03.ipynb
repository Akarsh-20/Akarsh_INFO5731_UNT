{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "\"Sentiment analysis on Apartments.com reviews\"\n",
        "To build a machine learning model for sentiment analysis on Apartments.com reviews\n",
        "These feature might be useful for building machine learning model\n",
        "\n",
        "Bag of Words (BoW):\n",
        "Explanation: BoW is a simple and commonly used text representation technique. It converts a piece of text into a numerical vector by counting the frequency of each word in the text and ignoring the order and structure of words.\n",
        "How it's Helpful: BoW helps capture the presence and frequency of specific words in reviews. It's useful for identifying important keywords or phrases in the text that may be indicative of sentiment. For example, words like \"excellent\" or \"disappointing\" can strongly influence sentiment classification.\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency):\n",
        "Explanation: TF-IDF is a statistical measure that evaluates the importance of a word within a document relative to a collection of documents (corpus). It considers both the frequency of a word in a review and its rarity across all reviews.\n",
        "How it's Helpful: TF-IDF is useful for identifying words that are discriminative for sentiment. It gives higher weight to words that are unique to certain reviews, helping to identify sentiment-bearing terms that are not common across all reviews.\n",
        "\n",
        "N-grams:\n",
        "Explanation: N-grams are contiguous sequences of N words from the text. For example, bigrams (N=2) capture pairs of adjacent words.\n",
        "How it's Helpful: N-grams capture more complex relationships between words and phrases. This can be valuable for understanding sentiment nuances. For example, the bigram \"not good\" can convey negative sentiment even if the individual words \"not\" and \"good\" are neutral.\n",
        "\n",
        "Sentiment Lexicons:\n",
        "Explanation: Sentiment lexicons are lists of words or phrases pre-labeled with sentiment (e.g., positive, negative, or neutral). Examples include the Harvard General Inquirer or the SentiWordNet lexicon.\n",
        "How it's Helpful: Sentiment lexicons provide a direct mapping of words to sentiment, allowing the model to quickly identify and weigh sentiment-bearing terms in the text. They can be valuable for fine-grained sentiment analysis by assigning sentiment scores to words.\n",
        "\n",
        "Part-of-Speech (POS) Tags:\n",
        "Explanation: POS tagging involves labeling words in a text with their corresponding grammatical categories (e.g., nouns, verbs, adjectives). For sentiment analysis, you might focus on adjectives and adverbs.\n",
        "How it's Helpful: Analyzing POS tags can help identify words that play a crucial role in expressing sentiment. Adjectives and adverbs often carry sentiment-related information. For instance, \"great\" and \"horrible\" are adjectives that strongly indicate sentiment.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f93d9ba-bb2e-45df-f61b-976324832702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Features:\n",
            "[[1 0 0 0 1 2 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0\n",
            "  1 1 0 1 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  1 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 1 1 1 0 2 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0\n",
            "  1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 2 1 0 0 1 1 0 0 0\n",
            "  0 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 3 0\n",
            "  0 0 1 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 1\n",
            "  0 2 0 0 1 0 1 0 1 0 1 0 1 2 0 0 0 0 2 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0\n",
            "  0 1 0 0 1 0 0 0 0 0 1 2 1 2 0 0 0 1 1 0 1 0 0 0 1 0 1 1 0 0 0 0 6 1 3 1\n",
            "  0 0 0 0 2 0 3 1 1 0 1 1]]\n",
            "\n",
            "TF-IDF Features:\n",
            "[[0.16389866 0.         0.         0.         0.16389866 0.17105824\n",
            "  0.         0.         0.         0.16389866 0.16389866 0.16389866\n",
            "  0.         0.16389866 0.         0.16389866 0.         0.\n",
            "  0.         0.         0.16389866 0.16389866 0.         0.\n",
            "  0.         0.16389866 0.16389866 0.16389866 0.         0.\n",
            "  0.16389866 0.         0.         0.         0.16389866 0.\n",
            "  0.         0.         0.         0.16389866 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.25843918\n",
            "  0.         0.         0.         0.16389866 0.         0.16389866\n",
            "  0.         0.16389866 0.16389866 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.16389866\n",
            "  0.16389866 0.         0.         0.         0.         0.\n",
            "  0.         0.16389866 0.16389866 0.16389866 0.         0.\n",
            "  0.         0.         0.         0.16389866 0.16389866 0.\n",
            "  0.         0.         0.         0.         0.         0.16389866\n",
            "  0.         0.         0.         0.         0.16389866 0.\n",
            "  0.16389866 0.         0.12921959 0.         0.10461439 0.\n",
            "  0.12921959 0.16389866 0.         0.16389866 0.         0.\n",
            "  0.         0.         0.         0.16389866 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.14831099\n",
            "  0.         0.22407204 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.28420698 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.22407204 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.22407204\n",
            "  0.         0.         0.         0.         0.28420698 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.28420698 0.         0.         0.\n",
            "  0.         0.         0.         0.56841395 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.28420698 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.28420698\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.22407204 0.         0.         0.         0.         0.22407204\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.14993073 0.14993073 0.14993073 0.         0.15648015\n",
            "  0.         0.         0.14993073 0.         0.         0.\n",
            "  0.14993073 0.         0.14993073 0.         0.         0.11820711\n",
            "  0.14993073 0.         0.         0.         0.         0.14993073\n",
            "  0.14993073 0.         0.         0.         0.         0.\n",
            "  0.         0.14993073 0.14993073 0.14993073 0.         0.\n",
            "  0.14993073 0.         0.14993073 0.         0.         0.14993073\n",
            "  0.11820711 0.14993073 0.         0.14993073 0.11820711 0.\n",
            "  0.         0.         0.14993073 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.11820711\n",
            "  0.         0.14993073 0.         0.29986145 0.14993073 0.\n",
            "  0.         0.14993073 0.14993073 0.         0.         0.\n",
            "  0.         0.         0.14993073 0.14993073 0.         0.14993073\n",
            "  0.14993073 0.         0.         0.         0.         0.\n",
            "  0.         0.11820711 0.14993073 0.         0.         0.\n",
            "  0.         0.14993073 0.         0.         0.14993073 0.\n",
            "  0.         0.14993073 0.         0.         0.         0.\n",
            "  0.         0.14993073 0.         0.         0.28709652 0.\n",
            "  0.         0.         0.14993073 0.         0.         0.11820711\n",
            "  0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.0539973\n",
            "  0.10347452 0.0815805  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.10347452 0.0815805\n",
            "  0.         0.10347452 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.10347452 0.20694904\n",
            "  0.         0.         0.         0.         0.         0.10347452\n",
            "  0.         0.163161   0.         0.         0.10347452 0.\n",
            "  0.0815805  0.         0.10347452 0.         0.0815805  0.\n",
            "  0.10347452 0.20694904 0.         0.         0.         0.\n",
            "  0.20694904 0.         0.         0.10347452 0.10347452 0.0815805\n",
            "  0.10347452 0.         0.         0.         0.         0.10347452\n",
            "  0.10347452 0.         0.         0.         0.10347452 0.\n",
            "  0.         0.10347452 0.         0.         0.10347452 0.\n",
            "  0.         0.         0.         0.         0.10347452 0.20694904\n",
            "  0.10347452 0.163161   0.         0.         0.         0.10347452\n",
            "  0.10347452 0.         0.10347452 0.         0.         0.\n",
            "  0.10347452 0.         0.10347452 0.10347452 0.         0.\n",
            "  0.         0.         0.48948299 0.10347452 0.19813933 0.10347452\n",
            "  0.         0.         0.         0.         0.20694904 0.\n",
            "  0.31042356 0.10347452 0.10347452 0.         0.10347452 0.10347452]]\n",
            "\n",
            "Bigram Features:\n",
            "[[1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0\n",
            "  0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1\n",
            "  0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0\n",
            "  0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0\n",
            "  0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0\n",
            "  0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1\n",
            "  1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0\n",
            "  1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 2 1 1 1 0 0 0 0\n",
            "  1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
            "  0 0 1 1 1 1 1 0 1 1]]\n",
            "\n",
            "Sentiment Lexicon Features:\n",
            "Positive Counts: [0, 0, 0, 0]\n",
            "Negative Counts: [0, 0, 0, 0]\n",
            "\n",
            "POS Tag Features:\n",
            "Adjective Counts: [6, 1, 8, 2]\n",
            "Adverb Counts: [1, 0, 6, 5]\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample reviews on Apartments.com\n",
        "reviews = [\n",
        "    \"the amenities' such as walk in closets, space up to date appliances. upgrade property, Courtesy officers, and beautiful bathrooms, and pantry in kitchen, with accessible covered parking, clubhouse, gas/electric fireplace, landscaping, pool, package items secure at leasing office.\",\n",
        "    \"I was looking for apartments in Spencerport NY and Rochester NY keeps coming up\",\n",
        "    \"App was good, but could be more user friendly by only recommendation places that fit search filters, not giving more expensive and other location options. Also, couldn’t figure out how to go back and select a place to add to my list if I accidentally “noped” it.\",\n",
        "    \"I don't know if this is a part of you or the people who list the apartments but I'd rather see on line first what I'd pay for the place before I go tour the apartment. I know to some degree what I want and what I can pay for so don't want to get myself into a place I'll have to leave the next year when the rent is raised....\",\n",
        "]\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_reviews = [word_tokenize(review.lower()) for review in reviews]\n",
        "\n",
        "# Bag of Words (BoW) feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "bow_features = vectorizer.fit_transform([' '.join(tokens) for tokens in tokenized_reviews])\n",
        "print(\"BoW Features:\")\n",
        "print(bow_features.toarray())\n",
        "\n",
        "# TF-IDF feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform([' '.join(tokens) for tokens in tokenized_reviews])\n",
        "print(\"\\nTF-IDF Features:\")\n",
        "print(tfidf_features.toarray())\n",
        "\n",
        "# N-grams feature extraction (bigrams)\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram_features = ngram_vectorizer.fit_transform([' '.join(tokens) for tokens in tokenized_reviews])\n",
        "print(\"\\nBigram Features:\")\n",
        "print(ngram_features.toarray())\n",
        "\n",
        "# Sentiment Lexicons\n",
        "positive_words = [\"amazing\", \"fantastic\", \"great\"]\n",
        "negative_words = [\"terrible\", \"boring\", \"awful\"]\n",
        "positive_counts = [sum(review.count(word) for word in positive_words) for review in reviews]\n",
        "negative_counts = [sum(review.count(word) for word in negative_words) for review in reviews]\n",
        "print(\"\\nSentiment Lexicon Features:\")\n",
        "print(\"Positive Counts:\", positive_counts)\n",
        "print(\"Negative Counts:\", negative_counts)\n",
        "\n",
        "# Part-of-Speech (POS) Tagging\n",
        "pos_tags = [pos_tag(tokens) for tokens in tokenized_reviews]\n",
        "adjective_counts = [len([tag for word, tag in tags if tag.startswith('JJ')]) for tags in pos_tags]\n",
        "adverb_counts = [len([tag for word, tag in tags if tag.startswith('RB')]) for tags in pos_tags]\n",
        "print(\"\\nPOS Tag Features:\")\n",
        "print(\"Adjective Counts:\", adjective_counts)\n",
        "print(\"Adverb Counts:\", adverb_counts)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0d7e2f-8c04-401e-95c5-1619ad9772ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chi-squared Scores by Label:\n",
            "BoW: 248.90476190476193\n",
            "TF-IDF: 33.084105729558644\n",
            "Positive Count: 1.0\n",
            "Negative Count: 1.0\n",
            "Adjective Count: 1.0\n",
            "Adverb Count: 1.0\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "\n",
        "# Sample class labels for sentiment classes (positive, negative, neutral)\n",
        "labels = [\"positive\", \"negative\", \"positive\", \"neutral\"]\n",
        "\n",
        "# Combine all the features into one feature matrix\n",
        "all_features = np.hstack([bow_features.toarray(), tfidf_features.toarray(),\n",
        "                          ngram_features.toarray(),\n",
        "                          np.array(positive_counts)[:, np.newaxis],\n",
        "                          np.array(negative_counts)[:, np.newaxis],\n",
        "                          np.array(adjective_counts)[:, np.newaxis],\n",
        "                          np.array(adverb_counts)[:, np.newaxis]])\n",
        "\n",
        "# Calculate Chi-squared statistics and p-values for each feature\n",
        "chi2_stat, p_values = chi2(all_features, labels)\n",
        "\n",
        "# Create a list of labels corresponding to each feature\n",
        "feature_labels = [\"BoW\"] * len(vectorizer.get_feature_names_out()) + \\\n",
        "                 [\"TF-IDF\"] * len(tfidf_vectorizer.get_feature_names_out()) + \\\n",
        "                 [\"Positive Count\", \"Negative Count\", \"Adjective Count\", \"Adverb Count\"]\n",
        "\n",
        "# Combine feature labels and Chi-squared scores\n",
        "feature_info = list(zip(feature_labels, chi2_stat))\n",
        "\n",
        "# Create a dictionary to accumulate the Chi-squared scores for each label\n",
        "label_scores = {}\n",
        "for label, score in feature_info:\n",
        "    if label in label_scores:\n",
        "        label_scores[label] += score\n",
        "    else:\n",
        "        label_scores[label] = score\n",
        "\n",
        "# Print the aggregated Chi-squared scores by label\n",
        "print(\"Chi-squared Scores by Label:\")\n",
        "for label, score in label_scores.items():\n",
        "    print(f\"{label}: {score}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03cad35-4a68-4738-f1f8-6039ab0e0bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Text Data (Descending Order of Cosine_Similarity):\n",
            "Similarity: 0.0848\n",
            "This website is the best to search for apartments\n",
            "\n",
            "Similarity: 0.0831\n",
            "It's neither good or bad\n",
            "\n",
            "Similarity: 0.0648\n",
            "Site is good, but loading times are slow\n",
            "\n",
            "Similarity: 0.0000\n",
            "Very bad site, no proper information available\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample text data\n",
        "text_data = [\n",
        "    \"This website is the best to search for apartments\",\n",
        "    \"Very bad site, no proper information available\",\n",
        "    \"Site is good, but loading times are slow\",\n",
        "    \"It's neither good or bad\",\n",
        "]\n",
        "\n",
        "\n",
        "query = \"Want to find a good apartment with all amenities\"\n",
        "all_text = text_data + [query]\n",
        "\n",
        "# Convert the text data into TF-IDF vectors\n",
        "count_vectorizer = CountVectorizer()\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_matrix = tfidf_transformer.fit_transform(count_vectorizer.fit_transform(all_text))\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
        "\n",
        "# Rank texts based on cosine similarities in descending order\n",
        "sorted_indices = cosine_similarities.argsort()[0][::-1]\n",
        "\n",
        "# Print the ranked text data based on cosine similarity\n",
        "print(\"Ranked Text Data (Descending Order of Cosine_Similarity):\")\n",
        "for idx in sorted_indices:\n",
        "    print(f\"Similarity: {cosine_similarities[0][idx]:.4f}\")\n",
        "    print(text_data[idx])\n",
        "    print()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}